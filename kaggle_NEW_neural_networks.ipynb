{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.747503Z",
     "iopub.status.busy": "2023-05-05T22:10:20.747112Z",
     "iopub.status.idle": "2023-05-05T22:10:20.782027Z",
     "shell.execute_reply": "2023-05-05T22:10:20.780858Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.747470Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# new torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # new vision-dataset-related torch imports\n",
    "# import torchvision\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# misc imports\n",
    "import random\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainstorm:\n",
    "\n",
    "1. Before Training: Use PCA to reduce the number of features (Low priority)\n",
    "\n",
    "    1.1 Things such as the dog ID should not make a difference\n",
    "\n",
    "2. Training: \n",
    "\n",
    "    2.1 Use deep neutral network to train a model and make predictions\n",
    "    \n",
    "    2.2 Use boosting and bagging to improve the model (Low priority)\n",
    "\n",
    "3. Things to consider:\n",
    "\n",
    "    3.1 Need to choose loss functions\n",
    "    \n",
    "    3.2 We should numericalize and normalize all features. Need to come up with ways to represent things such as \"date of birth\" and \"N/A\" data entries as numbers.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.811365Z",
     "iopub.status.busy": "2023-05-05T22:10:20.811001Z",
     "iopub.status.idle": "2023-05-05T22:10:20.873478Z",
     "shell.execute_reply": "2023-05-05T22:10:20.872591Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.811337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose from LF, LH, RF, RH \n",
    "leg = \"RF\"\n",
    "\n",
    "x_file_name = leg + \"_train.csv\"\n",
    "\n",
    "x_lf = pd.read_csv(x_file_name)\n",
    "\n",
    "y_lf = x_lf[leg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_my_data_bro(x_lf, leg = \"POTATO\", data_type = \"test\"):\n",
    "    \n",
    "    if data_type==\"test\":\n",
    "        x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\"], axis=1) # these should not matter\n",
    "    elif data_type==\"train\":\n",
    "        x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\",leg], axis=1) # these should not matter\n",
    "    else:\n",
    "        print(\"wrong data_type bro\")\n",
    "        \n",
    "    \n",
    "    # if gait is walk then 1 else -1\n",
    "    for i in range(0,len(x_lf['gait'])):\n",
    "        if x_lf['gait'][i] == \"Walk\":\n",
    "            x_lf['gait'][i] = 1\n",
    "        else:\n",
    "            x_lf['gait'][i]= -1\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(x_lf['speed'])):\n",
    "        if x_lf['speed'][i] == \"Not able to walk\":\n",
    "            x_lf['speed'][i] = 0\n",
    "        if x_lf['speed'][i] == \"Not able to trot\":\n",
    "            x_lf['speed'][i] = -1\n",
    "\n",
    "    x_lf = np.array(x_lf)\n",
    "            \n",
    "    col_remove = []\n",
    "    print(\"x_training col number \", len(x_lf[0]))\n",
    "    \n",
    "    \n",
    "    for j in range(0,len(x_lf[0])):\n",
    "\n",
    "        col_sum =0\n",
    "        col_count =0\n",
    "\n",
    "        for i in range(0,len(x_lf)):\n",
    "            \n",
    "            if j==185:\n",
    "                if x_lf[i][j]==\"Trot\":\n",
    "                    x_lf[i][j]=1\n",
    "            \n",
    "            if x_lf[i][j]==\"Not able to trot\":\n",
    "                x_lf[i][j]=float(\"nan\")\n",
    "        \n",
    "            if x_lf[i][j]==\"no valid trials\":\n",
    "                x_lf[i][j]= float(\"nan\")\n",
    "        \n",
    "            if x_lf[i][j] == \"No Data\" or x_lf[i][j]==\"no data\":\n",
    "                x_lf[i][j] = float(\"nan\")\n",
    "\n",
    "\n",
    "            if isinstance(x_lf[i][j], str)==True:\n",
    "                print(\"String, \" + str(x_lf[i][j]) +\", is converted to float\")\n",
    "                x_lf[i][j] = float(x_lf[i][j])    \n",
    "\n",
    "\n",
    "            \n",
    "            if np.isnan(x_lf[i][j])==False:\n",
    "                col_sum += x_lf[i][j]\n",
    "                col_count += 1\n",
    "\n",
    "\n",
    "        col_mean = col_sum/col_count\n",
    "        col_var = 0\n",
    "        \n",
    "        \n",
    "\n",
    "        for i in range(0,len(x_lf)):\n",
    "            if np.isnan(x_lf[i][j]):\n",
    "                #print(\"Nan, \" + str(x_lf[i][j]) +\", is set to the column mean\")\n",
    "                x_lf[i][j]=col_mean\n",
    "                \n",
    "            else:  \n",
    "                col_var += (x_lf[i][j]-col_mean)**2\n",
    "            \n",
    "\n",
    "        col_std = (col_var/float(col_count))**(1/2)\n",
    "\n",
    "        if col_std == 0:\n",
    "            col_remove.append(j) \n",
    "            col_std = 1\n",
    "            print(\"removed column \", j, \" bc all data are the same\")\n",
    "\n",
    "        x_lf[:,j] -= col_mean\n",
    "        x_lf[:,j] /= col_std\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in col_remove:\n",
    "        x_lf = np.delete(x_lf,obj=i,axis=1)\n",
    "\n",
    "    print(\"x_lf and y_lf now has \" + str(len(x_lf[0])) + \" columns\")\n",
    "\n",
    "    x_lf2 = x_lf # Cleaned_Data\n",
    "    \n",
    "    # CLIPPING\n",
    "    \n",
    "    x_lf2 = np.clip(x_lf2,-2,2)\n",
    "    \n",
    "    return x_lf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.875535Z",
     "iopub.status.busy": "2023-05-05T22:10:20.875032Z",
     "iopub.status.idle": "2023-05-05T22:10:20.891596Z",
     "shell.execute_reply": "2023-05-05T22:10:20.890701Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.875506Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\",\"RH\"], axis=1) # these should not matter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.904444Z",
     "iopub.status.busy": "2023-05-05T22:10:20.904075Z",
     "iopub.status.idle": "2023-05-05T22:10:20.909241Z",
     "shell.execute_reply": "2023-05-05T22:10:20.908038Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.904413Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in x_lf.columns:\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_training col number  367\n",
      "removed column  3  bc all data are the same\n",
      "String, 0.978, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 0.8699999999999999, is converted to float\n",
      "String, 1.05, is converted to float\n",
      "String, 1.12, is converted to float\n",
      "String, 1.0483333333333333, is converted to float\n",
      "String, 1.1575, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1.1275, is converted to float\n",
      "String, 1.43, is converted to float\n",
      "String, 1.1, is converted to float\n",
      "String, 1.0059999999999998, is converted to float\n",
      "String, 0.8200000000000001, is converted to float\n",
      "String, 0.9433333333333334, is converted to float\n",
      "String, 1.122, is converted to float\n",
      "String, 1.315, is converted to float\n",
      "String, 0.76, is converted to float\n",
      "String, 1.0619999999999998, is converted to float\n",
      "String, 1.0933333333333333, is converted to float\n",
      "String, 1.0799999999999998, is converted to float\n",
      "String, 1.1225, is converted to float\n",
      "String, 0.99, is converted to float\n",
      "String, 1.02, is converted to float\n",
      "String, 1.142, is converted to float\n",
      "String, 0.95, is converted to float\n",
      "String, 1.1375, is converted to float\n",
      "String, 1.06, is converted to float\n",
      "String, 1.188, is converted to float\n",
      "String, 1.052, is converted to float\n",
      "String, 1.17, is converted to float\n",
      "String, 1.12, is converted to float\n",
      "String, 1.3425, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1.266, is converted to float\n",
      "String, 1.18, is converted to float\n",
      "String, 1.222, is converted to float\n",
      "String, 1.18, is converted to float\n",
      "String, 1.1883333333333332, is converted to float\n",
      "String, 1.19, is converted to float\n",
      "String, 1.032, is converted to float\n",
      "String, 1.1974999999999998, is converted to float\n",
      "String, 1.0875, is converted to float\n",
      "String, 0.85, is converted to float\n",
      "String, 1.232, is converted to float\n",
      "String, 1.032, is converted to float\n",
      "String, 1.32, is converted to float\n",
      "String, 1.092, is converted to float\n",
      "String, 1.145, is converted to float\n",
      "String, 1.1575, is converted to float\n",
      "String, 1.1566666666666665, is converted to float\n",
      "String, 1.1066666666666667, is converted to float\n",
      "String, 1.2266666666666666, is converted to float\n",
      "String, 1.14, is converted to float\n",
      "String, 0.865, is converted to float\n",
      "String, 1.0779999999999998, is converted to float\n",
      "String, 1.29, is converted to float\n",
      "String, 1.2149999999999999, is converted to float\n",
      "String, 1.152, is converted to float\n",
      "String, 0.2, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1.1150000000000002, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 0.924, is converted to float\n",
      "String, 1.0799999999999998, is converted to float\n",
      "String, 1.0020000000000002, is converted to float\n",
      "String, 0.91, is converted to float\n",
      "String, 1.081429, is converted to float\n",
      "String, 0.96, is converted to float\n",
      "String, 1.1085714285714285, is converted to float\n",
      "String, 1.22, is converted to float\n",
      "String, 1.124, is converted to float\n",
      "String, 1.15, is converted to float\n",
      "String, 1.1400000000000001, is converted to float\n",
      "String, 1.1, is converted to float\n",
      "String, 1.136, is converted to float\n",
      "String, 1.0925, is converted to float\n",
      "String, 1.085, is converted to float\n",
      "String, 0.968, is converted to float\n",
      "String, 1.1975, is converted to float\n",
      "String, 0.91, is converted to float\n",
      "String, 0.9560000000000001, is converted to float\n",
      "String, 1.1119999999999999, is converted to float\n",
      "String, 1.38, is converted to float\n",
      "String, 0.9525, is converted to float\n",
      "String, 1.2233333333333334, is converted to float\n",
      "String, 1.01, is converted to float\n",
      "String, 1.025, is converted to float\n",
      "String, 1.12, is converted to float\n",
      "String, 1.2975, is converted to float\n",
      "String, 1.25, is converted to float\n",
      "String, 1.055, is converted to float\n",
      "String, 1.1099999999999999, is converted to float\n",
      "String, 1.014, is converted to float\n",
      "String, 1.07, is converted to float\n",
      "String, 1.1233333333333333, is converted to float\n",
      "String, 1.138, is converted to float\n",
      "String, 1.066, is converted to float\n",
      "String, 0.95, is converted to float\n",
      "String, 1.13, is converted to float\n",
      "String, 0.9480000000000001, is converted to float\n",
      "String, 1.0939999999999999, is converted to float\n",
      "String, 0.9900000000000002, is converted to float\n",
      "String, 1.0466666666666666, is converted to float\n",
      "String, 1.22, is converted to float\n",
      "removed column  185  bc all data are the same\n",
      "String, 1.9485714285714286, is converted to float\n",
      "String, 1.8840000000000003, is converted to float\n",
      "String, 2.17, is converted to float\n",
      "String, 1.5, is converted to float\n",
      "String, 1.99, is converted to float\n",
      "String, 1.96, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 2.028, is converted to float\n",
      "String, 1.99, is converted to float\n",
      "String, 2.054, is converted to float\n",
      "String, 2.1079999999999997, is converted to float\n",
      "String, 1.71, is converted to float\n",
      "String, 2.23, is converted to float\n",
      "String, 1.7579999999999998, is converted to float\n",
      "String, 1.8800000000000001, is converted to float\n",
      "String, 2.102, is converted to float\n",
      "String, 1.8975, is converted to float\n",
      "String, 2.07, is converted to float\n",
      "String, 1.843333333333333, is converted to float\n",
      "String, 1.975, is converted to float\n",
      "String, 1.738, is converted to float\n",
      "String, 2.26, is converted to float\n",
      "String, 2.165, is converted to float\n",
      "String, 1.748, is converted to float\n",
      "String, 2.1, is converted to float\n",
      "String, 1.796, is converted to float\n",
      "String, 2.3775000000000004, is converted to float\n",
      "String, 1.96, is converted to float\n",
      "String, 1.818, is converted to float\n",
      "String, 2.0159999999999996, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 1.9560000000000002, is converted to float\n",
      "String, 1.9, is converted to float\n",
      "String, 2.0100000000000002, is converted to float\n",
      "String, 2.142, is converted to float\n",
      "String, 1.8566666666666667, is converted to float\n",
      "String, 2.026, is converted to float\n",
      "String, 2.1525, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 2.09, is converted to float\n",
      "String, 1.845, is converted to float\n",
      "String, 1.67, is converted to float\n",
      "String, 2.206, is converted to float\n",
      "String, 1.9, is converted to float\n",
      "String, 2.01, is converted to float\n",
      "String, 1.906666666666667, is converted to float\n",
      "String, 2.168, is converted to float\n",
      "String, 2.1333333333333333, is converted to float\n",
      "String, 1.916, is converted to float\n",
      "String, 2.0300000000000002, is converted to float\n",
      "String, 2.27, is converted to float\n",
      "String, 2.41, is converted to float\n",
      "String, 1.688, is converted to float\n",
      "String, 1.47, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 2.0366666666666666, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 2.15, is converted to float\n",
      "String, 1.885, is converted to float\n",
      "String, 2.0025, is converted to float\n",
      "String, 1.63, is converted to float\n",
      "String, 1.834, is converted to float\n",
      "String, 1.8820000000000001, is converted to float\n",
      "String, 2.086, is converted to float\n",
      "String, 2.49, is converted to float\n",
      "String, 2.254, is converted to float\n",
      "String, 2.01, is converted to float\n",
      "String, 1.812, is converted to float\n",
      "String, 1.77, is converted to float\n",
      "String, 2.028, is converted to float\n",
      "String, 2.2, is converted to float\n",
      "String, 2.061666666666666, is converted to float\n",
      "String, 2.024, is converted to float\n",
      "String, 1.83, is converted to float\n",
      "String, 2.06, is converted to float\n",
      "String, 2.07, is converted to float\n",
      "String, 1.584, is converted to float\n",
      "String, 2.19, is converted to float\n",
      "String, 2.1979999999999995, is converted to float\n",
      "String, 2.22, is converted to float\n",
      "String, 2.2725, is converted to float\n",
      "String, 2.1275, is converted to float\n",
      "String, 1.885, is converted to float\n",
      "String, 1.97, is converted to float\n",
      "String, 1.8933333333333335, is converted to float\n",
      "String, 2.06, is converted to float\n",
      "String, 1.785, is converted to float\n",
      "String, 1.985, is converted to float\n",
      "String, 1.776, is converted to float\n",
      "String, 1.61, is converted to float\n",
      "String, 1.9549999999999998, is converted to float\n",
      "String, 1.9080000000000001, is converted to float\n",
      "String, 1.972, is converted to float\n",
      "x_lf and y_lf now has 365 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-188-80c2d53724cf>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['gait'][i] = 1\n",
      "<ipython-input-188-80c2d53724cf>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['speed'][i] = 0\n"
     ]
    }
   ],
   "source": [
    "x_lf2 = clean_up_my_data_bro(x_lf, leg=leg, data_type = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in x_lf2:\n",
    "#     for j in i:\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.140719Z",
     "iopub.status.busy": "2023-05-05T22:10:21.140350Z",
     "iopub.status.idle": "2023-05-05T22:10:21.314984Z",
     "shell.execute_reply": "2023-05-05T22:10:21.313742Z",
     "shell.execute_reply.started": "2023-05-05T22:10:21.140690Z"
    }
   },
   "outputs": [],
   "source": [
    "# columns_to_plot = 70\n",
    "\n",
    "# x = np.arange(0,len(x_lf2))\n",
    "# y = x_lf2[:,columns_to_plot]\n",
    "\n",
    "# plt.scatter(x,y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.317421Z",
     "iopub.status.busy": "2023-05-05T22:10:21.317116Z"
    }
   },
   "outputs": [],
   "source": [
    "# let labels be -1 and 1\n",
    "\n",
    "\n",
    "# for i in range(0,len(y_lf)):\n",
    "#     if y_lf[i]==0:\n",
    "#         y_lf[i]=-1\n",
    "\n",
    "y_lf2 = np.array(y_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1):\n",
    "        super(MLPNet, self).__init__()\n",
    "        \"\"\" pytorch optimizer checks for the properties of the model, and if\n",
    "            the torch.nn.Parameter requires gradient, then the model will update\n",
    "            the parameters automatically.\n",
    "        \"\"\"\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_probs(probs):\n",
    "    probs = torch.exp(probs)\n",
    "    sum_probs = torch.sum(probs, axis=-1, keepdims=True)\n",
    "    probs = probs/sum_probs\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training function\n",
    "# #<GRADED>\n",
    "# def train_classification_model( x, y, model, num_epochs, lr=1e-1, print_freq=100):\n",
    "#     \"\"\"Train loop for a neural network model. Please use the SGD optimizer, optim.SGD.\n",
    "    \n",
    "#     Input:\n",
    "#         train_loader:    Data loader for the train set. \n",
    "#                          Enumerate through to train with each batch.\n",
    "#         model:           nn.Model to be trained\n",
    "#         num_epochs:      number of epochs to train the model for\n",
    "#         lr:              learning rate for the optimizer\n",
    "#         print_freq:      frequency to display the loss\n",
    "    \n",
    "#     Output:\n",
    "#         model:   nn.Module trained model\n",
    "#     \"\"\"\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr)  # create an SGD optimizer for the model parameters\n",
    "#     for epoch in range(num_epochs):\n",
    "#         loss = 0\n",
    "\n",
    "#         # Iterate through the dataloader for each epoch\n",
    "#         #for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "#         for i in range(len(x)):\n",
    "#             # imgs (torch.Tensor):    batch of input images\n",
    "#             # labels (torch.Tensor):  batch labels corresponding to the inputs\n",
    "            \n",
    "#             # Implement the training loop using imgs, labels, and cross entropy loss\n",
    "            \n",
    "#             imgs = torch.tensor(x[i].astype(np.float32))\n",
    "#             labels = torch.tensor(y[i].astype(np.float32)).long()\n",
    "            \n",
    "            \n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             preds = model(imgs)\n",
    "#             preds = softmax_probs(preds)\n",
    "#             loss += nn.functional.cross_entropy(preds, labels)\n",
    "#             #loss = mse_loss(preds, labels)\n",
    "# #             loss = nn.BCELoss(preds, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "            \n",
    "            \n",
    "#         if (epoch + 1) % print_freq == 0:\n",
    "#             print(preds)\n",
    "#             print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "#         loss = 0\n",
    "#     return model  # return trained model\n",
    "# #</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(x, y, model, num_epochs, lr=1e-1, print_freq=100, batch_size=10):\n",
    "    \"\"\"Train loop for a neural network model. Please use the SGD optimizer, optim.SGD.\n",
    "    \n",
    "    Input:\n",
    "        train_loader:    Data loader for the train set. \n",
    "                         Enumerate through to train with each batch.\n",
    "        model:           nn.Model to be trained\n",
    "        num_epochs:      number of epochs to train the model for\n",
    "        lr:              learning rate for the optimizer\n",
    "        print_freq:      frequency to display the loss\n",
    "    \n",
    "    Output:\n",
    "        model:   nn.Module trained model\n",
    "    \"\"\"\n",
    "    n, d = x.shape\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)  # create an SGD optimizer for the model parameters\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_ids = np.random.choice(n, batch_size, replace=False)\n",
    "        x_batch = torch.tensor(x[batch_ids].astype(np.float32))\n",
    "        y_batch = torch.tensor(y[batch_ids].astype(np.float32)).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_batch)\n",
    "        preds = softmax_probs(preds)\n",
    "        loss = nn.functional.cross_entropy(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "#             print(preds)\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "            \n",
    "    return model  # return trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    square_diff = torch.pow((y_pred-y_true), 2)\n",
    "    mean_error = 0.5 * torch.mean(square_diff)\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "#<GRADED>\n",
    "def test_classification_model(x,y, model):\n",
    "    \"\"\"Tests the accuracy of the model.\n",
    "    \n",
    "    Input:\n",
    "        test_loader:      Data loader for the test set. \n",
    "                          Enumerate through to test each example.\n",
    "        model:            nn.Module model being evaluate.\n",
    "        \n",
    "    Output:\n",
    "        accuracy:         Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Compute the model accuracy\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    numer = 0 \n",
    "    demon = 0\n",
    "    \n",
    "    preds = model(torch.tensor(x.astype(np.float32)))\n",
    "    pred_lf = softmax_probs(preds)\n",
    "    results = torch.argmax(pred_lf, axis=1).numpy()\n",
    "    print(results)\n",
    "    print(y)\n",
    "    numer = np.sum(results == y)\n",
    "    denom = len(y)\n",
    "    return numer / denom\n",
    "    \n",
    "    \n",
    "    #for batch_idx, (imgs, labels) in enumerate(test_loader):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        vector = torch.tensor(x[i].astype(np.float32))\n",
    "        label = torch.tensor(y[i].astype(np.float32)).long()\n",
    "        \n",
    "        pred = model(vector)\n",
    "        \n",
    "        #preds = np.round(preds.numpy())\n",
    "        \n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        numer+= np.sum(preds.numpy() == labels)\n",
    "        demon+= np.size(preds.numpy())\n",
    "\n",
    "    accuracy = numer/demon\n",
    "    \n",
    "    \n",
    "    return accuracy\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_training col number  367\n",
      "removed column  3  bc all data are the same\n",
      "String, 1.162, is converted to float\n",
      "String, 0.9339999999999999, is converted to float\n",
      "String, 1.052, is converted to float\n",
      "String, 1.1675, is converted to float\n",
      "String, 0.8866666666666667, is converted to float\n",
      "String, 0.99, is converted to float\n",
      "String, 1.0663636363636364, is converted to float\n",
      "String, 1.0375, is converted to float\n",
      "String, 1.07, is converted to float\n",
      "String, 1.005, is converted to float\n",
      "String, 1.02, is converted to float\n",
      "String, 0.962, is converted to float\n",
      "String, 1.274, is converted to float\n",
      "String, 1.06, is converted to float\n",
      "String, 1.125, is converted to float\n",
      "String, 1.2200000000000002, is converted to float\n",
      "String, 1.1, is converted to float\n",
      "String, 1.162, is converted to float\n",
      "String, 1.1775000000000002, is converted to float\n",
      "String, 1.24, is converted to float\n",
      "String, 0.922, is converted to float\n",
      "String, 1.07, is converted to float\n",
      "String, 1.22, is converted to float\n",
      "String, 1.115, is converted to float\n",
      "String, 1.046, is converted to float\n",
      "String, 1.034, is converted to float\n",
      "String, 1.0550000000000002, is converted to float\n",
      "String, 0.95, is converted to float\n",
      "String, 1.33, is converted to float\n",
      "String, 1.04, is converted to float\n",
      "String, 1.1733333333333331, is converted to float\n",
      "String, 1.0819999999999999, is converted to float\n",
      "String, 1.1900000000000002, is converted to float\n",
      "String, 1.27, is converted to float\n",
      "String, 1.152, is converted to float\n",
      "String, 1.192, is converted to float\n",
      "String, 1.07, is converted to float\n",
      "String, 1.16, is converted to float\n",
      "String, 0.99, is converted to float\n",
      "String, 1.042, is converted to float\n",
      "String, 0.8966666666666666, is converted to float\n",
      "String, 1.014, is converted to float\n",
      "String, 1.09, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1.2374999999999998, is converted to float\n",
      "String, 1.084, is converted to float\n",
      "String, 1.046, is converted to float\n",
      "String, 1.005, is converted to float\n",
      "String, 1.192, is converted to float\n",
      "String, 1.2120000000000002, is converted to float\n",
      "String, 0.9366666666666666, is converted to float\n",
      "String, 1.046, is converted to float\n",
      "String, 1.4, is converted to float\n",
      "String, 1.092, is converted to float\n",
      "String, 0.9959999999999999, is converted to float\n",
      "String, 1.152, is converted to float\n",
      "String, 1.0514285714285714, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1.05, is converted to float\n",
      "String, 0.9233333333333333, is converted to float\n",
      "String, 1.0325, is converted to float\n",
      "String, 1.44, is converted to float\n",
      "String, 1, is converted to float\n",
      "String, 1.16, is converted to float\n",
      "String, 0.9500000000000001, is converted to float\n",
      "String, 1.082, is converted to float\n",
      "String, 1.205, is converted to float\n",
      "String, 0.96, is converted to float\n",
      "String, 0.9316666666666666, is converted to float\n",
      "String, 0.945, is converted to float\n",
      "String, 0.9925, is converted to float\n",
      "String, 1.05, is converted to float\n",
      "String, 0.975, is converted to float\n",
      "removed column  185  bc all data are the same\n",
      "String, 2.182, is converted to float\n",
      "String, 1.934, is converted to float\n",
      "String, 2.0966666666666662, is converted to float\n",
      "String, 1.89, is converted to float\n",
      "String, 1.8077777777777777, is converted to float\n",
      "String, 1.87, is converted to float\n",
      "String, 1.98, is converted to float\n",
      "String, 2.0460000000000003, is converted to float\n",
      "String, 1.76, is converted to float\n",
      "String, 2.19, is converted to float\n",
      "String, 2.105, is converted to float\n",
      "String, 1.9725, is converted to float\n",
      "String, 2.09, is converted to float\n",
      "String, 2.084, is converted to float\n",
      "String, 1.9275000000000002, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 1.8824999999999998, is converted to float\n",
      "String, 1.88, is converted to float\n",
      "String, 1.6239999999999999, is converted to float\n",
      "String, 2.262, is converted to float\n",
      "String, 2.096, is converted to float\n",
      "String, 2.09, is converted to float\n",
      "String, 1.9274999999999998, is converted to float\n",
      "String, 2.04, is converted to float\n",
      "String, 1.7127272727272724, is converted to float\n",
      "String, 1.8987499999999997, is converted to float\n",
      "String, 2.0300000000000002, is converted to float\n",
      "String, 1.8059999999999998, is converted to float\n",
      "String, 2.026, is converted to float\n",
      "String, 1.9966666666666668, is converted to float\n",
      "String, 1.7666666666666666, is converted to float\n",
      "String, 2.01, is converted to float\n",
      "String, 2.01, is converted to float\n",
      "String, 1.884, is converted to float\n",
      "String, 1.77, is converted to float\n",
      "String, 2.1425, is converted to float\n",
      "String, 1.9275, is converted to float\n",
      "String, 1.9100000000000001, is converted to float\n",
      "String, 1.608, is converted to float\n",
      "String, 2.034, is converted to float\n",
      "String, 1.63, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 2.65, is converted to float\n",
      "String, 2.15, is converted to float\n",
      "String, 1.925, is converted to float\n",
      "String, 1.6949999999999998, is converted to float\n",
      "String, 1.63, is converted to float\n",
      "String, 2.276, is converted to float\n",
      "String, 2.15, is converted to float\n",
      "String, 1.59, is converted to float\n",
      "String, 1.666, is converted to float\n",
      "String, 1.904, is converted to float\n",
      "String, 2.045, is converted to float\n",
      "String, 2.048, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 1.84, is converted to float\n",
      "String, 1.9925000000000002, is converted to float\n",
      "String, 2.09125, is converted to float\n",
      "String, 1.78, is converted to float\n",
      "String, 2, is converted to float\n",
      "String, 2.072, is converted to float\n",
      "String, 1.6280000000000001, is converted to float\n",
      "String, 1.9774999999999998, is converted to float\n",
      "String, 1.87, is converted to float\n",
      "String, 1.887777777777778, is converted to float\n",
      "String, 2.024, is converted to float\n",
      "String, 1.9, is converted to float\n",
      "String, 1.9140000000000001, is converted to float\n",
      "String, 2.18, is converted to float\n",
      "x_lf and y_lf now has 365 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-188-80c2d53724cf>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['gait'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "test_file_name = leg + \"_test.csv\"\n",
    "\n",
    "t_lf = pd.read_csv(test_file_name)\n",
    "\n",
    "t_id_arr = np.array(t_lf['id'])\n",
    "\n",
    "t_lf2 = clean_up_my_data_bro(t_lf, leg, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.5996981859207153\n",
      "epoch 200 loss 0.5579665899276733\n",
      "epoch 300 loss 0.5685879588127136\n",
      "epoch 400 loss 0.5303033590316772\n",
      "epoch 500 loss 0.5518680810928345\n",
      "epoch 600 loss 0.48009681701660156\n",
      "epoch 700 loss 0.4730663299560547\n",
      "epoch 800 loss 0.5400533080101013\n",
      "epoch 900 loss 0.5005539655685425\n",
      "epoch 1000 loss 0.5353165864944458\n",
      "epoch 1100 loss 0.37800896167755127\n",
      "epoch 1200 loss 0.4530049264431\n",
      "epoch 1300 loss 0.449384868144989\n",
      "epoch 1400 loss 0.4073735177516937\n",
      "epoch 1500 loss 0.4868205189704895\n",
      "epoch 1600 loss 0.4847831726074219\n",
      "epoch 1700 loss 0.6077994108200073\n",
      "epoch 1800 loss 0.5234719514846802\n",
      "epoch 1900 loss 0.48263120651245117\n",
      "epoch 2000 loss 0.5656778812408447\n",
      "epoch 2100 loss 0.5669005513191223\n",
      "epoch 2200 loss 0.5197942852973938\n",
      "epoch 2300 loss 0.3480152189731598\n",
      "epoch 2400 loss 0.43412846326828003\n",
      "epoch 2500 loss 0.43421459197998047\n",
      "epoch 2600 loss 0.4770529270172119\n",
      "epoch 2700 loss 0.4749320149421692\n",
      "epoch 2800 loss 0.47600287199020386\n",
      "epoch 2900 loss 0.5206952095031738\n",
      "epoch 3000 loss 0.3859495520591736\n",
      "epoch 3100 loss 0.3850261867046356\n",
      "epoch 3200 loss 0.4750438630580902\n",
      "epoch 3300 loss 0.42589837312698364\n",
      "epoch 3400 loss 0.473499596118927\n",
      "epoch 3500 loss 0.42500418424606323\n",
      "epoch 3600 loss 0.4268460273742676\n",
      "epoch 3700 loss 0.5178296566009521\n",
      "epoch 3800 loss 0.5629961490631104\n",
      "epoch 3900 loss 0.4219934046268463\n",
      "epoch 4000 loss 0.4267815947532654\n",
      "epoch 4100 loss 0.5162101984024048\n",
      "epoch 4200 loss 0.516538679599762\n",
      "epoch 4300 loss 0.3784056603908539\n",
      "epoch 4400 loss 0.510076105594635\n",
      "epoch 4500 loss 0.4699353277683258\n",
      "epoch 4600 loss 0.6000374555587769\n",
      "epoch 4700 loss 0.42047539353370667\n",
      "epoch 4800 loss 0.4208902418613434\n",
      "epoch 4900 loss 0.4575393795967102\n",
      "epoch 5000 loss 0.466450035572052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_dims = [10, 5]\n",
    "num_epochs = 5000\n",
    "lr = 1e-2\n",
    "\n",
    "x, y = x_lf2, y_lf2\n",
    "\n",
    "\n",
    "# n, d = x.shape\n",
    "# learn_boundary = int(n * 0.75)\n",
    "# permutation = np.random.permutation(n)\n",
    "# x_shuffled = x[permutation]\n",
    "# y_shuffled = y[permutation]\n",
    "# x_tr = x_shuffled[:learn_boundary]\n",
    "# y_tr = y_shuffled[:learn_boundary]\n",
    "# x_te = x_shuffled[learn_boundary:]\n",
    "# y_te = y_shuffled[learn_boundary:]\n",
    "\n",
    "\n",
    "mlp_model = MLPNet(input_dim=365, hidden_dims=hidden_dims, output_dim=2)\n",
    "# mlp_model = train_classification_model(x_tr, y_tr, mlp_model, num_epochs=num_epochs, lr=lr)\n",
    "mlp_model = train_classification_model(x_lf2, y_lf2, mlp_model, num_epochs=num_epochs, lr=lr,batch_size=20)\n",
    "\n",
    "# print('the number of parameters', sum(parameter.view(-1).size()[0] for parameter in mlp_model.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_classification_model(x_te, y_te, mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lf2= torch.tensor(t_lf2.astype(np.float32))\n",
    "\n",
    "pred_lf = mlp_model(t_lf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# print(y_lf2)\n",
    "# print(pred_lf)\n",
    "pred_lf = softmax_probs(pred_lf)\n",
    "# print(pred_lf)\n",
    "results = torch.argmax(pred_lf, axis=1).numpy()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final_lf  = np.sign(pred_lf.detach().numpy())\n",
    "\n",
    "# for i in range(0,len(final_lf)):\n",
    "#     final_lf[i]=int(float(final_lf[i]))\n",
    "#     if final_lf[i]==-1:\n",
    "#         final_lf[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lf = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Make Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "rows = zip(t_id_arr, final_lf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the tuples to a CSV file\n",
    "\n",
    "csv_file_name = leg + \"_test_labels.csv\"\n",
    "\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', leg]) # write header row\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub():\n",
    "\n",
    "    \"\"\"\n",
    "    This is a file that will help you convert your individual predictions to the final prediction. \n",
    "    In the same directory as this file, you should have the following 4 files:\n",
    "      - LF_test_labels.csv - with at least two columns, 'id' and 'LF'\n",
    "      - LH_test_labels.csv - with at least two columns, 'id' and 'LH'\n",
    "      - RF_test_labels.csv - with at least two columns, 'id' and 'RF'\n",
    "      - RH_test_labels.csv - with at least two columns, 'id' and 'RH'\n",
    "\n",
    "    Running this script will convert these four files into a single CSV file, submission.csv, by\n",
    "    mutating the IDs so that they also include the leg that is being checked.\n",
    "    \"\"\"\n",
    "\n",
    "    legs = [\"LF\", \"LH\", \"RF\", \"RH\"]\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for leg in legs:\n",
    "        # read in the file\n",
    "        test_prediction = pd.read_csv(f\"{leg}_test_labels.csv\")\n",
    "        # append the abbreviation for the leg\n",
    "        test_prediction['id'] = test_prediction['id'].astype(str) + f\"_{leg}\"\n",
    "        # rename the label column\n",
    "        test_prediction['label'] = test_prediction[leg]\n",
    "        # exclude any potential additional columns\n",
    "        dfs.append(test_prediction[['id', 'label']])\n",
    "\n",
    "    final_df = pd.concat(dfs)\n",
    "    final_df.to_csv(\"submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
