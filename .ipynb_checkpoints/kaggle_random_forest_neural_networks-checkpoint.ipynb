{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.747503Z",
     "iopub.status.busy": "2023-05-05T22:10:20.747112Z",
     "iopub.status.idle": "2023-05-05T22:10:20.782027Z",
     "shell.execute_reply": "2023-05-05T22:10:20.780858Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.747470Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# new torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # new vision-dataset-related torch imports\n",
    "# import torchvision\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# misc imports\n",
    "import random\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainstorm:\n",
    "\n",
    "1. Before Training: Use PCA to reduce the number of features (Low priority)\n",
    "\n",
    "    1.1 Things such as the dog ID should not make a difference\n",
    "\n",
    "2. Training: \n",
    "\n",
    "    2.1 Use deep neutral network to train a model and make predictions\n",
    "    \n",
    "    2.2 Use boosting and bagging to improve the model (Low priority)\n",
    "\n",
    "3. Things to consider:\n",
    "\n",
    "    3.1 Need to choose loss functions\n",
    "    \n",
    "    3.2 We should numericalize and normalize all features. Need to come up with ways to represent things such as \"date of birth\" and \"N/A\" data entries as numbers.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.811365Z",
     "iopub.status.busy": "2023-05-05T22:10:20.811001Z",
     "iopub.status.idle": "2023-05-05T22:10:20.873478Z",
     "shell.execute_reply": "2023-05-05T22:10:20.872591Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.811337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose from LF, LH, RF, RH \n",
    "leg = \"RH\"\n",
    "\n",
    "x_file_name = leg + \"_train.csv\"\n",
    "\n",
    "x_lf = pd.read_csv(x_file_name)\n",
    "\n",
    "y_lf = x_lf[leg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_my_data_bro(x_lf, leg = \"POTATO\", data_type = \"test\"):\n",
    "    \n",
    "    if data_type==\"test\":\n",
    "        x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\"], axis=1) # these should not matter\n",
    "    elif data_type==\"train\":\n",
    "        x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\",leg], axis=1) # these should not matter\n",
    "    else:\n",
    "        print(\"wrong data_type bro\")\n",
    "        \n",
    "    \n",
    "    # if gait is walk then 1 else -1\n",
    "    for i in range(0,len(x_lf['gait'])):\n",
    "        if x_lf['gait'][i] == \"Walk\":\n",
    "            x_lf['gait'][i] = 1\n",
    "        else:\n",
    "            x_lf['gait'][i]= -1\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(x_lf['speed'])):\n",
    "        if x_lf['speed'][i] == \"Not able to walk\":\n",
    "            x_lf['speed'][i] = 0\n",
    "        if x_lf['speed'][i] == \"Not able to trot\":\n",
    "            x_lf['speed'][i] = -1\n",
    "\n",
    "    x_lf = np.array(x_lf)\n",
    "            \n",
    "    col_remove = []\n",
    "    print(\"x_training col number \", len(x_lf[0]))\n",
    "    \n",
    "    \n",
    "    for j in range(0,len(x_lf[0])):\n",
    "\n",
    "        col_sum = 0\n",
    "        col_count =0\n",
    "\n",
    "        for i in range(0,len(x_lf)):\n",
    "            \n",
    "            # More Cleaning Lmao\n",
    "            if j==185:\n",
    "                if x_lf[i][j]==\"Trot\":\n",
    "                    x_lf[i][j]=1\n",
    "            \n",
    "            #if j==185: # NOT ONLY FOR 185\n",
    "            if x_lf[i][j]==\"Not able to trot\":\n",
    "                x_lf[i][j]=-1\n",
    "        \n",
    "            if x_lf[i][j]==\"no valid trials\":\n",
    "                x_lf[i][j]= float(\"nan\") # CHECK THIS\n",
    "        \n",
    "            if x_lf[i][j] == \"No Data\" or x_lf[i][j]==\"no data\":\n",
    "                x_lf[i][j] = float(\"nan\")\n",
    "\n",
    "\n",
    "            if isinstance(x_lf[i][j], str)==True:\n",
    "                #print(\"string found at \" + str(i) + \" \" +str(j))\n",
    "                x_lf[i][j] = float(x_lf[i][j])    \n",
    "\n",
    "\n",
    "            \n",
    "            if np.isnan(x_lf[i][j])==False:\n",
    "                col_sum += x_lf[i][j]\n",
    "                col_count += 1\n",
    "\n",
    "            \n",
    "            \n",
    "    #         Idea 1: only count not nan values\n",
    "    #         else:\n",
    "    #             col_sum += x_lf[i][j]\n",
    "    #             col_count += 1\n",
    "            # Idea 2: count them but if variance is zero, remove column\n",
    "\n",
    "        col_mean = col_sum/col_count\n",
    "        col_var = 0\n",
    "        \n",
    "        # this loop set nan to the mean (so 0 after normalized) and calculates variance\n",
    "        \n",
    "        for i in range(0,len(x_lf)):\n",
    "            if np.isnan(x_lf[i][j]):\n",
    "                x_lf[i][j]=col_mean\n",
    "            col_var += (x_lf[i][j]-col_mean)**2\n",
    "            \n",
    "\n",
    "        col_std = (col_var/float(col_count))**(1/2)\n",
    "\n",
    "        if col_std == 0:\n",
    "            col_remove.append(j) \n",
    "            col_std = 1\n",
    "            print(\"removed column \", j, \" bc all data are the same\")\n",
    "\n",
    "        x_lf[:,j] -= col_mean\n",
    "        x_lf[:,j] /= col_std\n",
    "    \n",
    "    \n",
    "    \n",
    "    # I MADE THE APPROXIMATION THAT NAN=-1, CAN IMPROVE LATER BY LOOKING AT SPECIFIC COLUMNS\n",
    "    \n",
    "\n",
    "    for i in col_remove:\n",
    "        x_lf = np.delete(x_lf,obj=i,axis=1)\n",
    "\n",
    "    print(\"x_lf and y_lf now has \" + str(len(x_lf[0])) + \" columns\")\n",
    "\n",
    "\n",
    "    x_lf2 = x_lf # Cleaned_Data\n",
    "\n",
    "\n",
    "    return x_lf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.875535Z",
     "iopub.status.busy": "2023-05-05T22:10:20.875032Z",
     "iopub.status.idle": "2023-05-05T22:10:20.891596Z",
     "shell.execute_reply": "2023-05-05T22:10:20.890701Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.875506Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\",\"RH\"], axis=1) # these should not matter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.904444Z",
     "iopub.status.busy": "2023-05-05T22:10:20.904075Z",
     "iopub.status.idle": "2023-05-05T22:10:20.909241Z",
     "shell.execute_reply": "2023-05-05T22:10:20.908038Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.904413Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in x_lf.columns:\n",
    "#     print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_training col number  367\n",
      "removed column  3  bc all data are the same\n",
      "removed column  185  bc all data are the same\n",
      "x_lf and y_lf now has 365 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-9aae06521892>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['gait'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "x_lf2 = clean_up_my_data_bro(x_lf, leg=leg, data_type = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in x_lf2:\n",
    "#     for j in i:\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.140719Z",
     "iopub.status.busy": "2023-05-05T22:10:21.140350Z",
     "iopub.status.idle": "2023-05-05T22:10:21.314984Z",
     "shell.execute_reply": "2023-05-05T22:10:21.313742Z",
     "shell.execute_reply.started": "2023-05-05T22:10:21.140690Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.317421Z",
     "iopub.status.busy": "2023-05-05T22:10:21.317116Z"
    }
   },
   "outputs": [],
   "source": [
    "# let labels be -1 and 1\n",
    "\n",
    "\n",
    "# for i in range(0,len(y_lf)):\n",
    "#     if y_lf[i]==0:\n",
    "#         y_lf[i]=-1\n",
    "\n",
    "y_lf2 = np.array(y_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pytorch Multilayer Perceptron (MLP) Model\n",
    "#<GRADED>\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(MLPNet, self).__init__()\n",
    "        \"\"\" pytorch optimizer checks for the properties of the model, and if\n",
    "            the torch.nn.Parameter requires gradient, then the model will update\n",
    "            the parameters automatically.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Initialize the fully connected layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        layers.append(self.fc1)\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(self.fc2)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass, with ReLU non-linearities\n",
    "        \n",
    "#         x=torch.flatten(x,1)\n",
    "        \n",
    "        \n",
    "    \n",
    "        return self.model(x)\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "#<GRADED>\n",
    "def train_classification_model( x, y, model, num_epochs, lr=1e-1, print_freq=100):\n",
    "    \"\"\"Train loop for a neural network model. Please use the SGD optimizer, optim.SGD.\n",
    "    \n",
    "    Input:\n",
    "        train_loader:    Data loader for the train set. \n",
    "                         Enumerate through to train with each batch.\n",
    "        model:           nn.Model to be trained\n",
    "        num_epochs:      number of epochs to train the model for\n",
    "        lr:              learning rate for the optimizer\n",
    "        print_freq:      frequency to display the loss\n",
    "    \n",
    "    Output:\n",
    "        model:   nn.Module trained model\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)  # create an SGD optimizer for the model parameters\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Iterate through the dataloader for each epoch\n",
    "        #for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "        for i in range(len(x)):\n",
    "            # imgs (torch.Tensor):    batch of input images\n",
    "            # labels (torch.Tensor):  batch labels corresponding to the inputs\n",
    "            \n",
    "            # Implement the training loop using imgs, labels, and cross entropy loss\n",
    "            \n",
    "            imgs = torch.tensor(x[i].astype(np.float32))\n",
    "            labels = torch.tensor(y[i].astype(np.float32)).long()\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            loss = nn.functional.cross_entropy(preds, labels)\n",
    "            #loss = mse_loss(preds, labels)\n",
    "#             loss = nn.BCELoss(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "\n",
    "    return model  # return trained model\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    square_diff = torch.pow((y_pred-y_true), 2)\n",
    "    mean_error = 0.5 * torch.mean(square_diff)\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "#<GRADED>\n",
    "def test_classification_model(x,y, model):\n",
    "    \"\"\"Tests the accuracy of the model.\n",
    "    \n",
    "    Input:\n",
    "        test_loader:      Data loader for the test set. \n",
    "                          Enumerate through to test each example.\n",
    "        model:            nn.Module model being evaluate.\n",
    "        \n",
    "    Output:\n",
    "        accuracy:         Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Compute the model accuracy\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    numer = 0 \n",
    "    demon = 0\n",
    "    \n",
    "    #for batch_idx, (imgs, labels) in enumerate(test_loader):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        imgs = torch.tensor(x[i].astype(np.float32))\n",
    "        labels = torch.tensor(y[i].astype(np.float32)).long()\n",
    "        \n",
    "        _, preds = torch.max(model(imgs),dim=1)\n",
    "        \n",
    "        #preds = np.round(preds.numpy())\n",
    "        \n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        numer+= np.sum(preds.numpy() == labels)\n",
    "        demon+= np.size(preds.numpy())\n",
    "\n",
    "    accuracy = numer/demon\n",
    "    \n",
    "    \n",
    "    return accuracy\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_training col number  367\n",
      "removed column  3  bc all data are the same\n",
      "removed column  185  bc all data are the same\n",
      "x_lf and y_lf now has 365 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-9aae06521892>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['gait'][i] = 1\n",
      "<ipython-input-49-9aae06521892>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['speed'][i] = 0\n"
     ]
    }
   ],
   "source": [
    "test_file_name = leg + \"_test.csv\"\n",
    "\n",
    "t_lf = pd.read_csv(test_file_name)\n",
    "\n",
    "t_id_arr = np.array(t_lf['id'])\n",
    "\n",
    "t_lf2 = clean_up_my_data_bro(t_lf, leg, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of parameters 7341\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-26693e0dadaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmlp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m365\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the number of parameters'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmlp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# avg_test_acc = test_classification_model(test_loader, mlp_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-bcc56a0d3dcc>\u001b[0m in \u001b[0;36mtrain_classification_model\u001b[0;34m(x, y, model, num_epochs, lr, print_freq)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m#loss = mse_loss(preds, labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#             loss = nn.BCELoss(preds, labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 1 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Train an MLP model on MNSIT\n",
    "hidden_dim = 20\n",
    "num_epochs = 2000\n",
    "lr = 1e-2\n",
    "\n",
    "\n",
    "\n",
    "mlp_model = MLPNet(input_dim=365, hidden_dim=hidden_dim, output_dim=1)\n",
    "print('the number of parameters', sum(parameter.view(-1).size()[0] for parameter in mlp_model.parameters()))\n",
    "mlp_model = train_classification_model(x_lf2, y_lf2, mlp_model, num_epochs=num_epochs, lr=lr)\n",
    "# avg_test_acc = test_classification_model(test_loader, mlp_model)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-30c6270ffeab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# t_lf2= torch.tensor(t_lf2.astype(np.float32))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_lf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_lf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-8b602bb7a805>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#</GRADED>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# t_lf2= torch.tensor(t_lf2.astype(np.float32))\n",
    "\n",
    "pred_lf = mlp_model(t_lf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
      " 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
      " 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_lf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  6.3990,  -5.7725],\n",
       "        [  3.1818,  -2.5331],\n",
       "        [  7.9552,  -7.0504],\n",
       "        [ -0.1595,   0.3985],\n",
       "        [ -5.7309,   5.6540],\n",
       "        [  1.5058,  -1.2356],\n",
       "        [  9.7951,  -8.3667],\n",
       "        [  6.0733,  -5.1298],\n",
       "        [  3.8111,  -3.0709],\n",
       "        [  5.1471,  -4.7137],\n",
       "        [ -6.3647,   6.1866],\n",
       "        [ -4.4884,   4.7405],\n",
       "        [  9.2765,  -9.2204],\n",
       "        [ -8.3650,   8.9724],\n",
       "        [  5.4265,  -5.0565],\n",
       "        [  6.7804,  -6.4609],\n",
       "        [ -4.0924,   3.9260],\n",
       "        [  3.1609,  -2.6148],\n",
       "        [ -1.7485,   1.3322],\n",
       "        [ -5.7132,   5.6058],\n",
       "        [ -7.9895,   7.9345],\n",
       "        [ -4.7650,   4.6476],\n",
       "        [  6.5293,  -5.5896],\n",
       "        [ -8.5913,   8.5257],\n",
       "        [  5.2381,  -4.7507],\n",
       "        [ -1.6505,   1.2332],\n",
       "        [  2.5466,  -2.1438],\n",
       "        [  1.8737,  -1.4162],\n",
       "        [  6.9478,  -5.9182],\n",
       "        [  2.6941,  -1.8633],\n",
       "        [  6.6928,  -6.1705],\n",
       "        [ -0.7852,   0.8992],\n",
       "        [  3.3893,  -2.8376],\n",
       "        [ -6.9020,   6.8001],\n",
       "        [  2.5884,  -2.9065],\n",
       "        [  3.0459,  -3.0378],\n",
       "        [ -6.8824,   7.0223],\n",
       "        [  2.4766,  -2.2868],\n",
       "        [  4.7559,  -4.6722],\n",
       "        [ -0.7753,   1.3923],\n",
       "        [  4.2969,  -4.1587],\n",
       "        [  2.6922,  -2.0551],\n",
       "        [  8.8815,  -7.5134],\n",
       "        [ -6.3699,   6.6640],\n",
       "        [  6.2513,  -5.4040],\n",
       "        [ -4.1302,   3.8819],\n",
       "        [  0.7133,  -1.0235],\n",
       "        [  4.1534,  -3.6903],\n",
       "        [  6.6590,  -5.8111],\n",
       "        [ -7.0807,   6.8873],\n",
       "        [  6.3205,  -6.9595],\n",
       "        [-14.1168,  14.0726],\n",
       "        [  0.9734,  -1.0903],\n",
       "        [  4.7950,  -4.3915],\n",
       "        [ -8.0246,   8.2359],\n",
       "        [  9.1400,  -8.0459],\n",
       "        [  6.5931,  -5.8720],\n",
       "        [  1.2124,  -0.9642],\n",
       "        [ 11.5928, -10.1446],\n",
       "        [  2.9088,  -2.5843],\n",
       "        [ -5.5275,   5.4153],\n",
       "        [ -3.2929,   2.7628],\n",
       "        [  2.3227,  -2.2928],\n",
       "        [  1.5531,  -1.6731],\n",
       "        [ -5.7478,   5.6984],\n",
       "        [  0.0537,   0.4305],\n",
       "        [ -0.1210,   0.4922],\n",
       "        [ -9.0409,   9.1407],\n",
       "        [  5.2188,  -4.2410],\n",
       "        [ -0.2822,   0.3234],\n",
       "        [  0.4467,  -0.8349],\n",
       "        [  5.2827,  -4.9100],\n",
       "        [  6.4147,  -6.1767],\n",
       "        [  7.7609,  -7.1414]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-b0ee374a56ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_lf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfinal_lf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_lf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_lf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfinal_lf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# final_lf  = np.sign(pred_lf.detach().numpy())\n",
    "\n",
    "# for i in range(0,len(final_lf)):\n",
    "#     final_lf[i]=int(float(final_lf[i]))\n",
    "#     if final_lf[i]==-1:\n",
    "#         final_lf[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lf = final_lf.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Make Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "rows = zip(t_id_arr, final_lf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the tuples to a CSV file\n",
    "\n",
    "csv_file_name = leg + \"_test_labels.csv\"\n",
    "\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', leg]) # write header row\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub():\n",
    "\n",
    "    \"\"\"\n",
    "    This is a file that will help you convert your individual predictions to the final prediction. \n",
    "    In the same directory as this file, you should have the following 4 files:\n",
    "      - LF_test_labels.csv - with at least two columns, 'id' and 'LF'\n",
    "      - LH_test_labels.csv - with at least two columns, 'id' and 'LH'\n",
    "      - RF_test_labels.csv - with at least two columns, 'id' and 'RF'\n",
    "      - RH_test_labels.csv - with at least two columns, 'id' and 'RH'\n",
    "\n",
    "    Running this script will convert these four files into a single CSV file, submission.csv, by\n",
    "    mutating the IDs so that they also include the leg that is being checked.\n",
    "    \"\"\"\n",
    "\n",
    "    legs = [\"LF\", \"LH\", \"RF\", \"RH\"]\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for leg in legs:\n",
    "        # read in the file\n",
    "        test_prediction = pd.read_csv(f\"{leg}_test_labels.csv\")\n",
    "        # append the abbreviation for the leg\n",
    "        test_prediction['id'] = test_prediction['id'].astype(str) + f\"_{leg}\"\n",
    "        # rename the label column\n",
    "        test_prediction['label'] = test_prediction[leg]\n",
    "        # exclude any potential additional columns\n",
    "        dfs.append(test_prediction[['id', 'label']])\n",
    "\n",
    "    final_df = pd.concat(dfs)\n",
    "    final_df.to_csv(\"submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
