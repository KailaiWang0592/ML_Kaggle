{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.747503Z",
     "iopub.status.busy": "2023-05-05T22:10:20.747112Z",
     "iopub.status.idle": "2023-05-05T22:10:20.782027Z",
     "shell.execute_reply": "2023-05-05T22:10:20.780858Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.747470Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainstorm:\n",
    "\n",
    "1. Before Training: Use PCA to reduce the number of features (Low priority)\n",
    "\n",
    "    1.1 Things such as the dog ID should not make a difference\n",
    "\n",
    "2. Training: \n",
    "\n",
    "    2.1 Use deep neutral network to train a model and make predictions\n",
    "    \n",
    "    2.2 Use boosting and bagging to improve the model (Low priority)\n",
    "\n",
    "3. Things to consider:\n",
    "\n",
    "    3.1 Need to choose loss functions\n",
    "    \n",
    "    3.2 We should numericalize and normalize all features. Need to come up with ways to represent things such as \"date of birth\" and \"N/A\" data entries as numbers.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.811365Z",
     "iopub.status.busy": "2023-05-05T22:10:20.811001Z",
     "iopub.status.idle": "2023-05-05T22:10:20.873478Z",
     "shell.execute_reply": "2023-05-05T22:10:20.872591Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.811337Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_lf = pd.read_csv(\"RH_train.csv\")\n",
    "\n",
    "y_lf = x_lf[\"RH\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_my_data_bro(x_lf,thing = \"test\"):\n",
    "    \n",
    "    if thing==\"test\":\n",
    "    \n",
    "        x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\"], axis=1) # these should not matter\n",
    "        \n",
    "    else:\n",
    "        x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\",\"RH\"], axis=1) # these should not matter\n",
    "    for i in range(0,len(x_lf['gait'])):\n",
    "        if x_lf['gait'][i] == \"Walk\":\n",
    "            x_lf['gait'][i] =1\n",
    "        else:\n",
    "            x_lf['gait'][i]=-1\n",
    "\n",
    "    for i in range(0,len(x_lf['speed'])):\n",
    "        if x_lf['speed'][i] == \"Not able to walk\":\n",
    "            x_lf['speed'][i] = -1\n",
    "        if x_lf['speed'][i] == \"Not able to trot\":\n",
    "            x_lf['speed'][i] = -1\n",
    "\n",
    "    x_lf = np.array(x_lf)\n",
    "            \n",
    "    col_remove = []\n",
    "    print(\"x_lf col number \", len(x_lf[0]))\n",
    "\n",
    "    for j in range(0,len(x_lf[0])):\n",
    "\n",
    "        col_sum = 0\n",
    "        col_count =0\n",
    "\n",
    "        for i in range(0,len(x_lf)):\n",
    "\n",
    "            # More Cleaning Lmao\n",
    "            if j==185:\n",
    "                if x_lf[i][j]==\"Trot\":\n",
    "                    x_lf[i][j]=1\n",
    "            #if j==185: # NOT ONLY FOR 185\n",
    "            if x_lf[i][j]==\"Not able to trot\":\n",
    "                x_lf[i][j]=-1\n",
    "        \n",
    "            if x_lf[i][j]==\"no valid trials\":\n",
    "                x_lf[i][j]=0 # CHECK THIS\n",
    "        \n",
    "            if x_lf[i][j] == \"No Data\" or x_lf[i][j]==\"no data\":\n",
    "                x_lf[i][j] = float(\"nan\")\n",
    "\n",
    "\n",
    "            if isinstance(x_lf[i][j], str)==True:\n",
    "                #print(\"string found at \" + str(i) + \" \" +str(j))\n",
    "                x_lf[i][j] = float(x_lf[i][j])    \n",
    "\n",
    "\n",
    "            if np.isnan(x_lf[i][j]):\n",
    "                x_lf[i][j]=-1\n",
    "    #         Idea 1: only count not nan values\n",
    "    #         else:\n",
    "    #             col_sum += x_lf[i][j]\n",
    "    #             col_count += 1\n",
    "            # Idea 2: count them but if variance is zero, remove column\n",
    "\n",
    "            col_sum += x_lf[i][j]\n",
    "            col_count += 1\n",
    "\n",
    "\n",
    "\n",
    "        col_mean = col_sum/col_count\n",
    "        col_var = 0\n",
    "        for i in range(0,len(x_lf)):\n",
    "            col_var += (x_lf[i][j]-col_mean)**2\n",
    "\n",
    "        col_std = (col_var/float(col_count))**(1/2)\n",
    "\n",
    "        if col_std == 0:\n",
    "            col_remove.append(j) \n",
    "            col_std = 1\n",
    "            print(\"removed column \", j, \" bc all data are the same\")\n",
    "\n",
    "        x_lf[:,j] -= col_mean\n",
    "        x_lf[:,j] /= col_std\n",
    "    \n",
    "    \n",
    "    \n",
    "    # I MADE THE APPROXIMATION THAT NAN=-1, CAN IMPROVE LATER BY LOOKING AT SPECIFIC COLUMNS\n",
    "    \n",
    "\n",
    "    for i in col_remove:\n",
    "        x_lf = np.delete(x_lf,obj=i,axis=1)\n",
    "\n",
    "    print(\"x_lf and y_lf now has \" + str(len(x_lf[0])) + \" columns\")\n",
    "\n",
    "\n",
    "    x_lf2 = x_lf # Cleaned_Data\n",
    "\n",
    "\n",
    "    return x_lf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.875535Z",
     "iopub.status.busy": "2023-05-05T22:10:20.875032Z",
     "iopub.status.idle": "2023-05-05T22:10:20.891596Z",
     "shell.execute_reply": "2023-05-05T22:10:20.890701Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.875506Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_lf = x_lf.drop(labels=[\"id\",\"dob\",\"forceplate_date\",\"RH\"], axis=1) # these should not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:20.904444Z",
     "iopub.status.busy": "2023-05-05T22:10:20.904075Z",
     "iopub.status.idle": "2023-05-05T22:10:20.909241Z",
     "shell.execute_reply": "2023-05-05T22:10:20.908038Z",
     "shell.execute_reply.started": "2023-05-05T22:10:20.904413Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in x_lf.columns:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.018201Z",
     "iopub.status.busy": "2023-05-05T22:10:21.017411Z",
     "iopub.status.idle": "2023-05-05T22:10:21.079649Z",
     "shell.execute_reply": "2023-05-05T22:10:21.078356Z",
     "shell.execute_reply.started": "2023-05-05T22:10:21.018164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a5e3abdcac52>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['gait'][i] =1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(x_lf['gait'])):\n",
    "    if x_lf['gait'][i] == \"Walk\":\n",
    "        x_lf['gait'][i] =1\n",
    "    else:\n",
    "        x_lf['gait'][i]=-1\n",
    "\n",
    "for i in range(0,len(x_lf['speed'])):\n",
    "    if x_lf['speed'][i] == \"Not able to walk\":\n",
    "        x_lf['speed'][i] = -1\n",
    "    if x_lf['speed'][i] == \"Not able to trot\":\n",
    "        x_lf['speed'][i] = -1\n",
    "# Isn't there smth else.. idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_lf col number  367\n",
      "removed column  3  bc all data are the same\n",
      "x_lf and y_lf now has 366 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-0770c962c127>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['gait'][i]=-1\n"
     ]
    }
   ],
   "source": [
    "x_lf2 = clean_up_my_data_bro(x_lf, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.085380Z",
     "iopub.status.busy": "2023-05-05T22:10:21.085032Z",
     "iopub.status.idle": "2023-05-05T22:10:21.092900Z",
     "shell.execute_reply": "2023-05-05T22:10:21.091848Z",
     "shell.execute_reply.started": "2023-05-05T22:10:21.085348Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Changing this little shit to numpy array\n",
    "# x_lf = np.array(x_lf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.140719Z",
     "iopub.status.busy": "2023-05-05T22:10:21.140350Z",
     "iopub.status.idle": "2023-05-05T22:10:21.314984Z",
     "shell.execute_reply": "2023-05-05T22:10:21.313742Z",
     "shell.execute_reply.started": "2023-05-05T22:10:21.140690Z"
    }
   },
   "outputs": [],
   "source": [
    "# col_remove = []\n",
    "# print(\"x_lf col number \", len(x_lf[0]))\n",
    "\n",
    "# for j in range(0,len(x_lf[0])):\n",
    "    \n",
    "#     col_sum = 0\n",
    "#     col_count =0\n",
    "\n",
    "#     for i in range(0,len(x_lf)):\n",
    "        \n",
    "#         # More Cleaning Lmao\n",
    "#         if j==185:\n",
    "#             if x_lf[i][j]==\"Trot\":\n",
    "#                 x_lf[i][j]=1\n",
    "#         #if j==185: # NOT ONLY FOR 185\n",
    "#         if x_lf[i][j]==\"Not able to trot\":\n",
    "#             x_lf[i][j]=-1\n",
    "        \n",
    "#         if x_lf[i][j]==\"no valid trials\":\n",
    "#                 x_lf[i][j]=0 # CHECK THIS\n",
    "        \n",
    "#         if x_lf[i][j] == \"No Data\" or x_lf[i][j]==\"no data\":\n",
    "#             x_lf[i][j] = float(\"nan\")\n",
    "            \n",
    "    \n",
    "#         if isinstance(x_lf[i][j], str)==True:\n",
    "#             #print(\"string found at \" + str(i) + \" \" +str(j))\n",
    "#             x_lf[i][j] = float(x_lf[i][j])    \n",
    "        \n",
    "        \n",
    "#         if np.isnan(x_lf[i][j]):\n",
    "#             x_lf[i][j]=-1\n",
    "# #         Idea 1: only count not nan values\n",
    "# #         else:\n",
    "# #             col_sum += x_lf[i][j]\n",
    "# #             col_count += 1\n",
    "#         # Idea 2: count them but if variance is zero, remove column\n",
    "    \n",
    "#         col_sum += x_lf[i][j]\n",
    "#         col_count += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "#     col_mean = col_sum/col_count\n",
    "#     col_var = 0\n",
    "#     for i in range(0,len(x_lf)):\n",
    "#         col_var += (x_lf[i][j]-col_mean)**2\n",
    "        \n",
    "#     col_std = (col_var/float(col_count))**(1/2)\n",
    "    \n",
    "#     if col_std == 0:\n",
    "#         col_remove.append(j) \n",
    "#         col_std = 1\n",
    "#         print(\"removed column \", j, \" bc all data are the same\")\n",
    "    \n",
    "#     x_lf[:,j] -= col_mean\n",
    "#     x_lf[:,j] /= col_std\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # I MADE THE APPROXIMATION THAT NAN=-1, CAN IMPROVE LATER BY LOOKING AT SPECIFIC COLUMNS\n",
    "    \n",
    "\n",
    "# for i in col_remove:\n",
    "#     x_lf = np.delete(x_lf,obj=i,axis=1)\n",
    "    \n",
    "# print(\"x_lf and y_lf now has \" + str(len(x_lf[0])) + \" columns\")\n",
    "\n",
    "\n",
    "# x_lf2 = x_lf # Cleaned_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T22:10:21.317421Z",
     "iopub.status.busy": "2023-05-05T22:10:21.317116Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-7e50fc914804>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_lf[i]=-1\n"
     ]
    }
   ],
   "source": [
    "# let labels be -1 and 1\n",
    "for i in range(0,len(y_lf)):\n",
    "    if y_lf[i]==0:\n",
    "        y_lf[i]=-1\n",
    "\n",
    "y_lf2 = np.array(y_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "class TreeNode(object):\n",
    "    \"\"\"Tree class.\n",
    "    \n",
    "    (You don't need to add any methods or fields here but feel\n",
    "    free to if you like. Our tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, parent, cutoff_id, cutoff_val, prediction):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.parent = parent\n",
    "        self.cutoff_id = cutoff_id\n",
    "        self.cutoff_val = cutoff_val\n",
    "        self.prediction = prediction\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def sqsplit(xTr,yTr,weights=[]):\n",
    "    \"\"\"Finds the best feature, cut value, and loss value.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     n x d matrix of data points\n",
    "        yTr:     n-dimensional vector of labels\n",
    "        weights: n-dimensional weight vector for data points\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: loss of the best cut\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_loss(w, mu, s):\n",
    "        return s - (mu ** 2) / w \n",
    "    \n",
    "    N,D = xTr.shape\n",
    "    assert D > 0 # must have at least one dimension\n",
    "    assert N > 1 # must have at least two samples\n",
    "    if weights == []: # if no weights are passed on, assign uniform weights\n",
    "        weights = np.ones(N)\n",
    "    weights = weights/sum(weights) # Weights need to sum to one (we just normalize them)\n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "    \n",
    "    for d in range(D):\n",
    "        ii = xTr[:,d].argsort() # sort data along the dth dimension\n",
    "        xs = xTr[ii,d] # sorted feature values\n",
    "        ws = weights[ii] # sorted weights\n",
    "        ys = yTr[ii] # sorted labels\n",
    "        \n",
    "        # Initialize constants\n",
    "        sL=0.0 # mean squared label on left side\n",
    "        muL=0.0 # mean label on left side\n",
    "        wL=0.0 # total weight on left side\n",
    "        sR=ws.dot(ys**2) #mean squared label on right \n",
    "        muR=ws.dot(ys) # mean label on right\n",
    "        wR=sum(ws) # weight on right\n",
    "        \n",
    "        # np.finfo(float).eps gives us the smallest possible positive number that can be represented by floats. \n",
    "        idif = np.where(np.abs(np.diff(xs, axis=0)) > np.finfo(float).eps * 100)[0]\n",
    "        pj = 0\n",
    "\n",
    "        for j in idif:\n",
    "            \n",
    "            # xs to move are in range(pj + 1, j + 1)\n",
    "#             if j - pj == 0: # moving exactly one data point, beginning of array\n",
    "#                 wL += ws[j] \n",
    "#                 muL += ys[j] * ws[j]\n",
    "#                 sL += ws[j] * (ys[j] ** 2)\n",
    "#                 wR -= ws[j]\n",
    "#                 muR -= ys[j] * ws[j]\n",
    "#                 sR -= ws[j] * (ys[j] ** 2)\n",
    "#             else:\n",
    "            ys_slice = ys[pj: j + 1]\n",
    "            ws_slice = ws[pj: j + 1]\n",
    "            ws_to_move = np.sum(ws_slice)\n",
    "            mus_to_move = np.dot(ys_slice, ws_slice)\n",
    "            ss_to_move = np.dot(ws_slice, np.power(ys_slice, 2))\n",
    "            wL += ws_to_move\n",
    "            wR -= ws_to_move\n",
    "            muL += mus_to_move\n",
    "            muR -= mus_to_move\n",
    "            sL += ss_to_move\n",
    "            sR -= ss_to_move\n",
    "            pj = j + 1 # don't forget to change pj in this case\n",
    "            lossL, lossR = compute_loss(wL, muL, sL), compute_loss(wR, muR, sR)\n",
    "            loss = lossL + lossR\n",
    "            if loss < bestloss:\n",
    "                feature = d\n",
    "                cut = (xs[j] + xs[j + 1]) / 2 # adding eps to ensure the last point is in the left partition\n",
    "                bestloss = loss\n",
    "        \n",
    "    assert feature != np.inf and cut != np.inf\n",
    "    \n",
    "    return feature, cut, bestloss\n",
    "    \n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "#<GRADED>\n",
    "def cart(xTr,yTr,depth=np.inf,weights=None):\n",
    "    \"\"\"Builds a CART tree.\n",
    "    \n",
    "    The maximum tree depth is defined by \"maxdepth\" (maxdepth=2 means one split).\n",
    "    Each example can be weighted with \"weights\".\n",
    "\n",
    "    Args:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "        maxdepth: maximum tree depth\n",
    "        weights:  n-dimensional weight vector for data points\n",
    "\n",
    "    Returns:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n,d = xTr.shape\n",
    "    if weights is None:\n",
    "        w = np.ones(n) / float(n)\n",
    "    else:\n",
    "        w = weights\n",
    "    \n",
    "    def build_cart(xTr, yTr, depth, weights, parent):\n",
    "        n, d = xTr.shape\n",
    "        pred = np.mean(yTr)\n",
    " \n",
    "        \n",
    "        label_same = np.all(yTr==yTr[0])\n",
    "        feature_same = np.all(xTr==xTr[0])\n",
    "        if depth <= 1 or label_same or feature_same:\n",
    "            return TreeNode(None, None, parent, None, None, pred)\n",
    "        else:\n",
    "            feature, cut, _ = sqsplit(xTr,yTr,weights)\n",
    "            ii = xTr[:,feature].argsort()\n",
    "            xs = xTr[ii,:] \n",
    "            ws = weights[ii]\n",
    "            ys = yTr[ii]\n",
    "            lefts = np.where(xs[:,feature] <= cut)[0]\n",
    "            rights = np.where(xs[:, feature] > cut)[0]\n",
    "            xTrL, wsL, yTrL = xs[lefts,:], ws[lefts], ys[lefts]\n",
    "            xTrR, wsR, yTrR = xs[rights,:], ws[rights], ys[rights]\n",
    "            depth -= 1\n",
    "            node = TreeNode(None, None, parent, feature, cut, pred)\n",
    "            left_child = build_cart(xTrL, yTrL, depth, wsL, node)\n",
    "            right_child = build_cart(xTrR, yTrR, depth, wsR, node)\n",
    "            node.left = left_child\n",
    "            node.right = right_child\n",
    "            return node\n",
    "            \n",
    "    return build_cart(xTr, yTr, depth, w, None)\n",
    "    \n",
    "    \n",
    "#</GRADED>\n",
    "\n",
    "#<GRADED>\n",
    "def forest(xTr, yTr, m, maxdepth=np.inf):\n",
    "    \"\"\"Creates a random forest.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data points\n",
    "        yTr:      n-dimensional vector of labels\n",
    "        m:        number of trees in the forest\n",
    "        maxdepth: maximum depth of tree\n",
    "        \n",
    "    Output:\n",
    "        trees: list of TreeNode decision trees of length m\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    trees = []\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        index = np.random.randint(low=0, high=n , size=n) # goes from 0 to n-1, n samples\n",
    "        \n",
    "        sampled_xTr = xTr[index]\n",
    "        sampled_yTr = yTr[index]\n",
    "        \n",
    "        result = cart(sampled_xTr, sampled_yTr, depth = maxdepth)\n",
    "        \n",
    "        trees.append(result)\n",
    "        \n",
    "    return trees\n",
    "#</GRADED>\n",
    "\n",
    "#<GRADED>\n",
    "def evaltree(root,xTe):\n",
    "    \"\"\"Evaluates xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        root: TreeNode decision tree\n",
    "        xTe:  n x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def evaltree_point(root, x):\n",
    "        if root.left is None:\n",
    "            return root.prediction\n",
    "        else:\n",
    "            if x[root.cutoff_id] < root.cutoff_val:\n",
    "                return evaltree_point(root.left, x)\n",
    "            else:\n",
    "                return evaltree_point(root.right, x)\n",
    "    \n",
    "    assert root is not None\n",
    "    n = xTe.shape[0]\n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        pred[i] = evaltree_point(root, xTe[i,:])\n",
    "\n",
    "    return pred\n",
    "#</GRADED>\n",
    "\n",
    "#<GRADED>\n",
    "def evalforest(trees, X, alphas=None):\n",
    "    \"\"\"Evaluates X using trees.\n",
    "    \n",
    "    Input:\n",
    "        trees:  list of TreeNode decision trees of length m\n",
    "        X:      n x d matrix of data points\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m = len(trees)\n",
    "    n,d = X.shape\n",
    "    if alphas is None:\n",
    "        alphas = np.ones(m) / m\n",
    "            \n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    for i in range(m):\n",
    "        preds = evaltree(trees[i], X)\n",
    "        pred += alphas[i] * preds\n",
    "    \n",
    "    return pred\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_lf col number  367\n",
      "removed column  3  bc all data are the same\n",
      "x_lf and y_lf now has 366 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-0770c962c127>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['gait'][i] =1\n",
      "<ipython-input-12-0770c962c127>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_lf['speed'][i] = -1\n"
     ]
    }
   ],
   "source": [
    "t_lf = pd.read_csv(\"RH_test.csv\")\n",
    "\n",
    "t_id_arr = np.array(t_lf['id'])\n",
    "\n",
    "t_lf2 = clean_up_my_data_bro(t_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-b9e4d48310ef>:22: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if weights == []: # if no weights are passed on, assign uniform weights\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e77871335541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmy_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-b9e4d48310ef>\u001b[0m in \u001b[0;36mforest\u001b[0;34m(xTr, yTr, m, maxdepth)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0msampled_yTr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myTr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_xTr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_yTr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b9e4d48310ef>\u001b[0m in \u001b[0;36mcart\u001b[0;34m(xTr, yTr, depth, weights)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b9e4d48310ef>\u001b[0m in \u001b[0;36mbuild_cart\u001b[0;34m(xTr, yTr, depth, weights, parent)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b9e4d48310ef>\u001b[0m in \u001b[0;36mbuild_cart\u001b[0;34m(xTr, yTr, depth, weights, parent)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b9e4d48310ef>\u001b[0m in \u001b[0;36mbuild_cart\u001b[0;34m(xTr, yTr, depth, weights, parent)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b9e4d48310ef>\u001b[0m in \u001b[0;36mbuild_cart\u001b[0;34m(xTr, yTr, depth, weights, parent)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myTr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxTr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxTr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b9e4d48310ef>\u001b[0m in \u001b[0;36msqsplit\u001b[0;34m(xTr, yTr, weights)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mws_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mws_to_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mmus_to_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mss_to_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mwL\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mws_to_move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxdepth = 100\n",
    "m=10\n",
    "\n",
    "my_forest = forest(x_lf2, y_lf2, m, maxdepth=maxdepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lf = evalforest(my_forest, t_lf2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_lf  = np.sign(pred_lf)\n",
    "\n",
    "for i in range(0,len(final_lf)):\n",
    "    final_lf[i]=int(float(final_lf[i]))\n",
    "    if final_lf[i]==-1:\n",
    "        final_lf[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lf = final_lf.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Make Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "rows = zip(t_id_arr, final_lf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the tuples to a CSV file\n",
    "with open('RH_test_labels.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', 'RH']) # write header row\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub():\n",
    "\n",
    "    \"\"\"\n",
    "    This is a file that will help you convert your individual predictions to the final prediction. \n",
    "    In the same directory as this file, you should have the following 4 files:\n",
    "      - LF_test_labels.csv - with at least two columns, 'id' and 'LF'\n",
    "      - LH_test_labels.csv - with at least two columns, 'id' and 'LH'\n",
    "      - RF_test_labels.csv - with at least two columns, 'id' and 'RF'\n",
    "      - RH_test_labels.csv - with at least two columns, 'id' and 'RH'\n",
    "\n",
    "    Running this script will convert these four files into a single CSV file, submission.csv, by\n",
    "    mutating the IDs so that they also include the leg that is being checked.\n",
    "    \"\"\"\n",
    "\n",
    "    legs = [\"LF\", \"LH\", \"RF\", \"RH\"]\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for leg in legs:\n",
    "        # read in the file\n",
    "        test_prediction = pd.read_csv(f\"{leg}_test_labels.csv\")\n",
    "        # append the abbreviation for the leg\n",
    "        test_prediction['id'] = test_prediction['id'].astype(str) + f\"_{leg}\"\n",
    "        # rename the label column\n",
    "        test_prediction['label'] = test_prediction[leg]\n",
    "        # exclude any potential additional columns\n",
    "        dfs.append(test_prediction[['id', 'label']])\n",
    "\n",
    "    final_df = pd.concat(dfs)\n",
    "    final_df.to_csv(\"submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
